{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5eb0586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import io\n",
    "import xarray as xr\n",
    "import os\n",
    "import numpy as np\n",
    "import metpy\n",
    "import metpy.calc as mpcalc\n",
    "from metpy.units import units\n",
    "from datetime import datetime\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.patches as mpatches\n",
    "import glob\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from olr_to_tb import *\n",
    "import math\n",
    "import wrf\n",
    "from netCDF4 import Dataset\n",
    "import fsspec\n",
    "import zarr\n",
    "from wrf import getvar\n",
    "import gc\n",
    "\n",
    "\n",
    "src_dir = r\"C:\\Users\\miles\\Downloads\\mcs_data\\unzipped\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78daba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset(\n",
    "    os.path.join(src_dir, \"*.nc\"),\n",
    "    engine=\"netcdf4\",\n",
    "    chunks={\"Time\": 1}, \n",
    "    concat_dim=\"Time\",\n",
    "    combine=\"nested\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6182b524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate avo (need to get from each file individually to use getvar)\n",
    "file_paths = sorted(glob.glob(os.path.join(src_dir, \"*.nc\")))\n",
    "avo_list = []\n",
    "cape_list = []\n",
    "ctt_list = []\n",
    "dbz_list = []\n",
    "helicity_list = []\n",
    "pvo_list = []\n",
    "td2_list = []\n",
    "updraft_helicity_list = []\n",
    "uvmet10_list = []\n",
    "uv_met_list = []\n",
    "temp_list = []\n",
    "slp_list = []\n",
    "p_list = []\n",
    "z_list = []\n",
    "i = 0\n",
    "\n",
    "for fpath in file_paths:\n",
    "    i+=1\n",
    "    print(i)\n",
    "    with Dataset(fpath) as ncfile:\n",
    "        avo = getvar(ncfile, \"avo\")\n",
    "        cape = getvar(ncfile, \"cape_2d\")\n",
    "        ctt = getvar(ncfile, \"ctt\")\n",
    "        dbz = getvar(ncfile, \"dbz\")\n",
    "        helicity = getvar(ncfile, \"helicity\")\n",
    "        pvo = getvar(ncfile, \"pvo\")\n",
    "        td2 = getvar(ncfile, \"td2\")\n",
    "        updraft_helicity = getvar(ncfile, \"updraft_helicity\")\n",
    "        uvmet10 = getvar(ncfile, \"uvmet10\")\n",
    "        uvmet = getvar(ncfile, \"uvmet\")\n",
    "        temp = getvar(ncfile, \"temp\")\n",
    "        slp = getvar(ncfile, \"slp\")\n",
    "        p = getvar(ncfile, \"pressure\")\n",
    "        z = getvar(ncfile, \"z\")\n",
    "\n",
    "        avo_list.append(avo.expand_dims(\"Time\"))\n",
    "        cape_list.append(cape.expand_dims(\"Time\"))\n",
    "        ctt_list.append(ctt.expand_dims(\"Time\"))\n",
    "        dbz_list.append(dbz.expand_dims(\"Time\"))\n",
    "        helicity_list.append(helicity.expand_dims(\"Time\"))\n",
    "        pvo_list.append(pvo.expand_dims(\"Time\"))\n",
    "        td2_list.append(td2.expand_dims(\"Time\"))\n",
    "        updraft_helicity_list.append(updraft_helicity.expand_dims(\"Time\"))\n",
    "        uvmet10_list.append(uvmet10.expand_dims(\"Time\"))\n",
    "        uv_met_list.append(uvmet.expand_dims(\"Time\"))\n",
    "        temp_list.append(temp.expand_dims(\"Time\"))\n",
    "        slp_list.append(slp.expand_dims(\"Time\"))\n",
    "        p_list.append(p.expand_dims(\"Time\"))\n",
    "        z_list.append(z.expand_dims(\"Time\"))\n",
    "        \n",
    "\n",
    "# Concatenate into a single DataArray\n",
    "avo_all = xr.concat(avo_list, dim=\"Time\")\n",
    "cape_all = xr.concat(cape_list, dim=\"Time\")\n",
    "ctt_all = xr.concat(ctt_list, dim=\"Time\")\n",
    "dbz_all = xr.concat(dbz_list, dim=\"Time\")\n",
    "helicity_all = xr.concat(helicity_list, dim=\"Time\")\n",
    "pvo_all = xr.concat(pvo_list, dim=\"Time\")\n",
    "td2_all = xr.concat(td2_list, dim=\"Time\")\n",
    "updraft_helicity_all = xr.concat(updraft_helicity_list, dim=\"Time\")\n",
    "uvmet10_all = xr.concat(uvmet10_list, dim=\"Time\")\n",
    "uv_met_all = xr.concat(uv_met_list, dim=\"Time\")\n",
    "temp_all = xr.concat(temp_list, dim =\"Time\")\n",
    "slp_all = xr.concat(slp_list, dim=\"Time\")\n",
    "p_all = xr.concat(p_list, dim=\"Time\")\n",
    "z_all = xr.concat(z_list, dim=\"Time\")\n",
    "\n",
    "ds[\"avo\"] = avo_all\n",
    "ds[\"cape\"] = cape_all.sel(mcape_mcin_lcl_lfc = 'mcape')\n",
    "ds[\"cin\"] = cape_all.sel(mcape_mcin_lcl_lfc = 'mcin')\n",
    "ds[\"lcl\"] = cape_all.sel(mcape_mcin_lcl_lfc = 'lcl')\n",
    "ds[\"lfc\"] = cape_all.sel(mcape_mcin_lcl_lfc = 'lfc')\n",
    "ds[\"ctt\"] = ctt_all\n",
    "ds[\"dbz\"] = dbz_all\n",
    "ds[\"helicity\"] = helicity_all\n",
    "ds[\"pvo\"] = pvo_all\n",
    "ds[\"td2\"] = td2_all\n",
    "ds[\"updraft_helicity\"] = updraft_helicity_all\n",
    "ds[\"u_met10\"] = uvmet10_all.sel(u_v = 'u')\n",
    "ds[\"v_met10\"] = uvmet10_all.sel(u_v = 'v')\n",
    "ds[\"u_met\"] = uv_met_all.sel(u_v = 'u')\n",
    "ds[\"v_met\"] = uv_met_all.sel(u_v = 'v')\n",
    "ds[\"temp\"] = temp_all\n",
    "ds[\"slp\"] = slp_all\n",
    "ds['avo_500'] = wrf.interplevel(avo_all, p_all, 500)\n",
    "ds['pvo_500'] = wrf.interplevel(pvo_all, p_all, 500)\n",
    "ds['avo_700'] = wrf.interplevel(avo_all, p_all, 700)\n",
    "ds['pvo_700'] = wrf.interplevel(pvo_all, p_all, 700)\n",
    "\n",
    "ds['u_300'] = wrf.interplevel(ds[\"u_met\"], p_all, 300)\n",
    "ds['v_300'] = wrf.interplevel(ds[\"v_met\"], p_all, 300)\n",
    "ds['z_300'] = wrf.interplevel(z_all, p_all, 300)\n",
    "\n",
    "ds['u_500'] = wrf.interplevel(ds[\"u_met\"], p_all, 500)\n",
    "ds['v_500'] = wrf.interplevel(ds[\"v_met\"], p_all, 500)\n",
    "ds['z_500'] = wrf.interplevel(z_all, p_all, 500)\n",
    "\n",
    "ds['u_700'] = wrf.interplevel(ds[\"u_met\"], p_all, 700)\n",
    "ds['v_700'] = wrf.interplevel(ds[\"v_met\"], p_all, 700)\n",
    "ds['z_700'] = wrf.interplevel(z_all, p_all, 700)\n",
    "\n",
    "ds['u_850'] = wrf.interplevel(ds[\"u_met\"], p_all, 850)\n",
    "ds['v_850'] = wrf.interplevel(ds[\"v_met\"], p_all, 850)\n",
    "ds['z_850'] = wrf.interplevel(z_all, p_all, 850)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4003cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIND SHEAR\n",
    "g = 9.81  # Acceleration due to gravity (m/s^2)\n",
    "z = (ds['PH'] + ds['PHB']) / g  # Geopotential height in meters\n",
    "z = 0.5 * (z[:, :-1, :, :] + z[:, 1:, :, :])\n",
    "z_target = 6000  # Target height in meters\n",
    "\n",
    "bottom_top_levels = ds['bottom_top_stag']\n",
    "z_diff = np.abs(z - z_target)\n",
    "z_index = z_diff.argmin(dim='bottom_top_stag')  # Index of closest level to 6 km\n",
    "\n",
    "def extract_level(var, level_idx):\n",
    "    return var[level_idx]\n",
    "\n",
    "# Apply to u and v wind\n",
    "u_6km = xr.apply_ufunc(\n",
    "    extract_level,\n",
    "    ds['u_met'], z_index,\n",
    "    input_core_dims=[['bottom_top'], []],\n",
    "    output_core_dims=[[]],\n",
    "    vectorize=True,\n",
    "    dask='parallelized',\n",
    "    output_dtypes=[ds['u_met'].dtype]\n",
    ")\n",
    "\n",
    "v_6km = xr.apply_ufunc(\n",
    "    extract_level,\n",
    "    ds['v_met'], z_index,\n",
    "    input_core_dims=[['bottom_top'], []],\n",
    "    output_core_dims=[[]],\n",
    "    vectorize=True,\n",
    "    dask='parallelized',\n",
    "    output_dtypes=[ds['v_met'].dtype]\n",
    ")\n",
    "\n",
    "u_0km = ds['u_met10'] \n",
    "v_0km = ds['v_met10']\n",
    "\n",
    "delta_u = u_6km.values - u_0km\n",
    "delta_v = v_6km.values - v_0km\n",
    "\n",
    "# Calculate wind shear magnitude\n",
    "shear = np.sqrt(np.square(delta_u) + np.square(delta_v))\n",
    "\n",
    "ds['U_shear_6'] = delta_u\n",
    "ds['V_shear_6'] = delta_v\n",
    "ds['mag_shear_6'] = shear\n",
    "\n",
    "# CLOUD HEIGHT\n",
    "\n",
    "temp_difs = np.abs(ds['temp'] - ds['ctt'] - 273.15)\n",
    "temp_index = temp_difs.argmin(dim='bottom_top')\n",
    "cloud_heights = z.isel(bottom_top_stag = temp_index.compute())\n",
    "ds['CL_HT'] = cloud_heights\n",
    "\n",
    "# RAIN\n",
    "ds['RAIN'] = ds['RAINC'] + ds['RAINNC']\n",
    "rains = np.zeros_like(ds['RAIN'])\n",
    "tot_rain = ds['RAINC'] + ds['RAINNC']\n",
    "rains[0, :, :] = np.zeros_like(ds['RAIN'].isel(Time = 0))\n",
    "rains[12, :, :] = np.zeros_like(ds['RAIN'].isel(Time = 0))\n",
    "for i in range(1, 12):\n",
    "    rains[i, :, :] = tot_rain.isel(Time = i) - tot_rain.isel(Time = i - 1)\n",
    "for i in range(13, 25):\n",
    "    rains[i, :, :] = tot_rain.isel(Time = i) - tot_rain.isel(Time = i - 1)\n",
    "ds['RAIN'] = (['Time', 'south_north', 'west_east'], rains)\n",
    "\n",
    "# other variables\n",
    "ds['U_1'] = ds['u_met'].isel(bottom_top = 10)\n",
    "ds['V_1'] = ds['v_met'].isel(bottom_top = 10)\n",
    "ds['WIND_sfc_mag'] = np.sqrt(np.square(ds['u_met10']) + np.square(ds['v_met10']))\n",
    "ds['theta'] = ds['T'] + ds['T00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc086fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "hails = np.zeros_like(ds['HAILNC'])\n",
    "tot_hail = ds['HAILNC']\n",
    "hails[0, :, :] = np.zeros_like(ds['HAILNC'].isel(Time = 0))\n",
    "hails[12, :, :] = np.zeros_like(ds['HAILNC'].isel(Time = 0))\n",
    "for i in range(1, 12):\n",
    "    hails[i, :, :] = tot_hail.isel(Time = i) - tot_hail.isel(Time = i - 1)\n",
    "for i in range(13, 25):\n",
    "    hails[i, :, :] = tot_hail.isel(Time = i) - tot_hail.isel(Time = i - 1)\n",
    "ds['HAIL'] = (['Time', 'south_north', 'west_east'], hails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7342a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def sanitize_attrs(attrs):\n",
    "    clean_attrs = {}\n",
    "    for k, v in attrs.items():\n",
    "        try:\n",
    "            json.dumps(v)  # Test if it's serializable\n",
    "            clean_attrs[k] = v\n",
    "        except (TypeError, ValueError):\n",
    "            print(f\"Removing non-serializable attribute: {k} = {v}\")\n",
    "    return clean_attrs\n",
    "\n",
    "# Clean dataset-level attrs\n",
    "ds.attrs = sanitize_attrs(ds.attrs)\n",
    "\n",
    "# Clean attrs for each variable\n",
    "for var in ds.variables:\n",
    "    ds[var].attrs = sanitize_attrs(ds[var].attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dade58",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['REFL_10CM_sfc'] = ds['REFL_10CM'].isel(bottom_top = 0)\n",
    "ds['theta_sfc'] = ds['theta'].isel(bottom_top = 0)\n",
    "ds['avo_sfc'] = ds['avo'].isel(bottom_top = 0)\n",
    "ds['pvo_sfc'] = ds['pvo'].isel(bottom_top = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075140b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_zarr(src_dir + '/processed_data.zarr', mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f30e76ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miles\\miniconda3\\envs\\mcs\\lib\\site-packages\\xarray\\conventions.py:289: SerializationWarning: variable 'dbz' has multiple fill values {1e+20, 1e+20} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "c:\\Users\\miles\\miniconda3\\envs\\mcs\\lib\\site-packages\\xarray\\conventions.py:289: SerializationWarning: variable 'pvo' has multiple fill values {1e+20, 1e+20} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "c:\\Users\\miles\\miniconda3\\envs\\mcs\\lib\\site-packages\\xarray\\conventions.py:289: SerializationWarning: variable 'pvo_sfc' has multiple fill values {1e+20, 1e+20} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "c:\\Users\\miles\\miniconda3\\envs\\mcs\\lib\\site-packages\\xarray\\conventions.py:289: SerializationWarning: variable 'td2' has multiple fill values {1e+20, 1e+20} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "c:\\Users\\miles\\miniconda3\\envs\\mcs\\lib\\site-packages\\xarray\\conventions.py:289: SerializationWarning: variable 'temp' has multiple fill values {1e+20, 1e+20} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n"
     ]
    }
   ],
   "source": [
    "# start here if file already exists\n",
    "ds = xr.open_zarr(src_dir + '/processed_data.zarr', chunks='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "956356e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in mrms: mask number 4 (both res). In WRF: mask number 6 for 4/3km and 7 for 4km.\n",
    "mask_dir = r\"C:\\Users\\miles\\Downloads\\mcs_masks\"\n",
    "filepaths = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir) if f.endswith(\".nc\")])\n",
    "\n",
    "# Open and concatenate along the time dimension\n",
    "masks = xr.open_mfdataset(\n",
    "    filepaths,\n",
    "    concat_dim=\"time\",\n",
    "    combine=\"nested\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99523f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_maps(ds, var, title, x_barb=None, y_barb=None, scale=1, shift=0, barbscale=1,\n",
    "              cmap='viridis', bottom=None, top=None, bottom_top=0, show=False, save=False,\n",
    "              save_loc=None, thin=30, zoom=None, feature_ds=None, feature_id=None):\n",
    "    \n",
    "    var_data = ds[var]\n",
    "    if x_barb:\n",
    "        u_data = ds[x_barb]\n",
    "        v_data = ds[y_barb]\n",
    "\n",
    "        if 'bottom_top' in u_data.dims:\n",
    "            u_data = u_data.sel(bottom_top=bottom_top)\n",
    "        if 'bottom_top' in v_data.dims:\n",
    "            v_data = v_data.sel(bottom_top=bottom_top)\n",
    "\n",
    "        thin_slice = slice(None, None, thin)\n",
    "        u_data = u_data.isel(south_north=thin_slice, west_east=thin_slice)\n",
    "        v_data = v_data.isel(south_north=thin_slice, west_east=thin_slice)\n",
    "        \n",
    "    else:\n",
    "        u_data = None\n",
    "        v_data = None\n",
    "\n",
    "    if zoom is not None:\n",
    "        var_data = var_data.where(\n",
    "            ((var_data.XLAT >= zoom[2]) & (var_data.XLAT <= zoom[3]) &\n",
    "            (var_data.XLONG >= zoom[0]) & (var_data.XLONG <= zoom[1])).compute(),\n",
    "            drop=True\n",
    "        )\n",
    "\n",
    "    if scale == None:\n",
    "        scale = 1\n",
    "    if shift == None:\n",
    "        shift = 0\n",
    "    if barbscale == None:\n",
    "        barbscale = 1\n",
    "    if cmap == 'grey_to_white':\n",
    "        cmap = plt.cm.gray  # Base colormap\n",
    "        gray_to_white = cmap(np.linspace(0.5, 1, 256))  # Use the upper half of the colormap\n",
    "        cmap = plt.matplotlib.colors.ListedColormap(gray_to_white)\n",
    "\n",
    "    if 'bottom_top' in var_data.dims:\n",
    "        global_min = var_data.isel(bottom_top=bottom_top).min().values*scale + shift\n",
    "        global_max = var_data.isel(bottom_top=bottom_top).max().values*scale + shift\n",
    "        var_data = var_data.isel(bottom_top=bottom_top)\n",
    "    else:\n",
    "        global_min = var_data.min().values*scale + shift\n",
    "        global_max = var_data.max().values*scale + shift\n",
    "    if bottom:\n",
    "        global_min = bottom\n",
    "    if top:\n",
    "        global_max = top\n",
    "\n",
    "    range_mag = 10 ** math.floor(math.log10(global_max - global_min))\n",
    "\n",
    "    # Round down min_val and up max_val to the nearest multiple of range_mag\n",
    "    global_min = math.floor(global_min / range_mag) * range_mag\n",
    "    global_max = math.ceil(global_max / range_mag) * range_mag\n",
    "\n",
    "    if bottom == 2000: # manual override for cloud heights\n",
    "        global_min = 2000\n",
    "    if var == 'updraft_helicity':\n",
    "        global_min = 0\n",
    "    if var == 'helicity':\n",
    "        global_min = -500\n",
    "        global_max = 500\n",
    "    if var == 'ctt':\n",
    "        global_min = -80\n",
    "        global_max = 20\n",
    "    if var == 'updraft_helicity':\n",
    "        global_min = 10\n",
    "        global_max = 110\n",
    "    if var in ['avo', 'avo_500', 'avo_700']:\n",
    "        global_min = -100\n",
    "        global_max = 100\n",
    "    if var == 'slp':\n",
    "        global_min = 995\n",
    "        global_max = 1020\n",
    "    if var in ['pvo', 'pvo_500', 'pvo_700']:\n",
    "        global_min = -40\n",
    "        global_max = 40\n",
    "\n",
    "    # Get coordinate names once, not every frame\n",
    "    lat_coord = [name for name in var_data.coords if \"LAT\" in name][0]\n",
    "    long_coord = [name for name in var_data.coords if \"LONG\" in name][0]\n",
    "    if x_barb:\n",
    "        ulat_coord = [name for name in u_data.coords if \"LAT\" in name][0]\n",
    "        ulong_coord = [name for name in u_data.coords if \"LONG\" in name][0]\n",
    "\n",
    "    projection = ccrs.LambertConformal()\n",
    "    coast = cfeature.COASTLINE.with_scale('50m')\n",
    "    states = cfeature.STATES.with_scale('50m')\n",
    "    borders = cfeature.BORDERS.with_scale('50m')\n",
    "\n",
    "\n",
    "    def process_timestep(i, var_data, lat_coord, long_coord, title,\n",
    "                     bottom_top, x_barb, u_data, v_data, zoom, cmap,\n",
    "                     scale, shift, barbscale, global_min, global_max,\n",
    "                     feature_ds, feature_id, save, save_loc, show, projection, coast, states, borders):\n",
    "    \n",
    "        time = var_data['XTIME'].isel(Time=i)\n",
    "        frame = var_data.isel(Time=i)\n",
    "        frame = (frame.load() * scale + shift)\n",
    "\n",
    "        if x_barb:\n",
    "        \n",
    "            frameu = u_data.isel(Time=i)\n",
    "            frameu = (frameu.load() * barbscale)\n",
    "\n",
    "            framev = v_data.isel(Time=i)\n",
    "            framev = (framev.load() * barbscale)      \n",
    "\n",
    "        fig = plt.figure(figsize=(16, 12))\n",
    "        ax = fig.add_subplot(111, projection=projection)\n",
    "\n",
    "        if zoom is None:\n",
    "            lon_min = frame[long_coord].min().item()\n",
    "            lon_max = frame[long_coord].max().item()\n",
    "            lat_min = frame[lat_coord].min().item()\n",
    "            lat_max = frame[lat_coord].max().item()\n",
    "            ax.set_extent([lon_min, lon_max, lat_min, lat_max])\n",
    "        else:\n",
    "            ax.set_extent(zoom)\n",
    "\n",
    "        ax.add_feature(coast, linewidth=1)\n",
    "        ax.add_feature(states, linewidth=0.5, edgecolor='black')\n",
    "        ax.add_feature(borders, linewidth=1, edgecolor='black')\n",
    "        \n",
    "\n",
    "        plt1 = ax.pcolormesh(\n",
    "            frame[long_coord], frame[lat_coord], frame, cmap=cmap,\n",
    "            transform=ccrs.PlateCarree(), vmin=global_min, vmax=global_max)\n",
    "        if x_barb:\n",
    "            ax.barbs(\n",
    "                frameu[ulong_coord].values, frameu[ulat_coord].values, frameu.values, framev.values, transform=ccrs.PlateCarree(), length = 6)\n",
    "        # Overlay feature outline if provided\n",
    "        if feature_ds is not None and feature_id is not None:\n",
    "            # Match time index\n",
    "            feature_time = feature_ds.sel(time=pd.to_datetime(str(time.values)), method='nearest')\n",
    "            \n",
    "            # Build mask for where tracknumber == feature_id\n",
    "            mask = (feature_time['tracknumber'] == feature_id)\n",
    "            mask_np = mask.squeeze().values\n",
    "            mask_data = np.where(mask_np, 1.0, 0.0)\n",
    "            \n",
    "            # Plot contour\n",
    "            ax.contour(\n",
    "                feature_time['lon'], feature_time['lat'], mask_data,\n",
    "                levels=[0.5], colors='black', linewidths=1, transform=ccrs.PlateCarree(), zorder=9)\n",
    "            ax.contour(\n",
    "                feature_time['lon'], feature_time['lat'], mask_data,\n",
    "                levels=[0.5], colors='white', linewidths=1, linestyles = 'dashed', transform=ccrs.PlateCarree(), zorder=10)\n",
    "        \n",
    "        cbar = fig.colorbar(plt1, ax=ax, orientation='horizontal', pad=0.05, aspect=30)\n",
    "        ax.set_title(\n",
    "            'WRF ' + title,\n",
    "            fontweight='bold', fontsize=14, loc='left')\n",
    "        #dt = datetime.utcfromtimestamp(time.values.astype(int) * 1e-9)\n",
    "        ax.set_title(time.values, fontsize=14, loc='right')\n",
    "        # # Format the gridlines (optional)\n",
    "        gl = ax.gridlines(\n",
    "            crs=ccrs.PlateCarree(), draw_labels=True, dms=True, x_inline=False,\n",
    "            y_inline=False, linewidth=1, color='k', linestyle=':')\n",
    "        gl.xlocator = mticker.FixedLocator([-130, -125, -120, -115, -110])\n",
    "        gl.ylocator = mticker.FixedLocator([40, 45, 50, 55])\n",
    "        gl.top_labels = False\n",
    "        gl.right_labels = False\n",
    "        gl.xlabel_style = {'size': 16, 'rotation': 20}\n",
    "        gl.ylabel_style = {'size': 16}\n",
    "        print(pd.to_datetime(str(time.values)).strftime('%d%H'))\n",
    "        if save:\n",
    "            if not os.path.exists(save_loc):\n",
    "                os.makedirs(save_loc)\n",
    "            plt.savefig(save_loc + pd.to_datetime(str(time.values)).strftime('%d%H') + '.png')\n",
    "            # print('saved in ' + save_loc + pd.to_datetime(str(time.values)).strftime('%d%H') + '.png')\n",
    "        if show:\n",
    "            plt.show()\n",
    "        \n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "        plt.close('all')\n",
    "        del mask, mask_np, mask_data, feature_time\n",
    "        del fig, ax, cbar, gl\n",
    "        del frame, plt1\n",
    "        if x_barb:\n",
    "            del frameu, framev\n",
    "        gc.collect()\n",
    "        \n",
    "    for i in range(len(var_data['XTIME'])):\n",
    "        process_timestep(i, var_data, lat_coord, long_coord, title,\n",
    "                        bottom_top, x_barb, u_data, v_data, zoom, cmap,\n",
    "                        scale, shift, barbscale, global_min, global_max,\n",
    "                        feature_ds, feature_id, save, save_loc, show, projection, coast, states, borders)\n",
    "\n",
    "\n",
    "#make_maps(combined_dataset, 'RAIN', 'Test', bottom = .1, top = 10, show = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1413fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gif(frame_folder):\n",
    "    frames = [Image.open(image) for image in glob.glob(f\"{frame_folder}/*.png\")]\n",
    "    frame_one = frames[0]\n",
    "    frame_one.save(f\"{frame_folder}/loop.gif\", format=\"GIF\", append_images=frames,\n",
    "               save_all=True, duration=300, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80116d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "td2 1 of 1\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n"
     ]
    }
   ],
   "source": [
    "vars_of_interest = ['REFL_10CM_sfc', 'OLR', 'T2', 'mag_shear_6', 'CL_HT', 'ctt', 'cape', 'RAIN', 'WIND_sfc_mag', 'theta_sfc', 'helicity', 'pvo_sfc', 'avo_sfc', 'cin', 'updraft_helicity', 'slp', 'lcl', 'avo_500', 'pvo_500', 'avo_700', 'pvo_700', 'z_300', 'z_500', 'z_700', 'z_850', 'td2'] \n",
    "titles = ['Reflectivity (DBZ)', 'Outgoing Longwave Radiation (W/m2)', \n",
    "          '2m Temperature (F) and Wind Barbs', '0-6 km Wind Shear (Magnitude in kt and Barbs)', 'Cloud Top Height (m; Derived from Brightness Temp)', \n",
    "          'Cloud Top Temperature (degC)', 'CAPE (J/kg) and 0-6 km Wind Shear Barbs', 'Hourly Rain (mm)', 'Surface Wind (kt)', 'Surface Potential Temperature (K) and Wind Barbs',\n",
    "          '0-3 km Storm Relative Helicity (m2/s2)', 'Potential Vorticity (PVU)', 'Absolute Vorticity (10-5 s-1)', 'CIN (J/kg)', 'Updraft Helicity (m2/s2)', 'Sea Level Pressure (hPa) and Surface Wind Barbs', 'Lifting Condensation Level (m)', '500 mb Absolute Vorticity (10-5 s-1)', '500 mb Potential Vorticity (PVU)', '700 mb Absolute Vorticity (10-5 s-1)', '700 mb Potential Vorticity (PVU)',\n",
    "          '300 mb Height (dam) and Wind (kt)', '500 mb Height (dam) and Wind (kt)', '700 mb Height (dam) and Wind (kt)', '850 mb Height (dam) and Wind (kt)', '2m Dew Point (F) and Surface Wind Barbs']\n",
    "\n",
    "x_barbs = [None, None, 'U10', 'U_shear_6', None, None, 'U_shear_6', None, 'U10', 'U10', None, None, None, None, None, 'U10', None, None, None, None, None, 'u_300', 'u_500', 'u_700', 'u_850', 'U10']\n",
    "y_barbs = [None, None,  'V10', 'V_shear_6', None, None, 'V_shear_6', None, 'V10', 'V10', None, None, None, None, None, 'V10', None, None, None, None, None, 'v_300', 'v_500', 'v_700', 'v_850', 'V10']\n",
    "save_folders = ['REFL_10CM',  'OLR', 'sfc_temp_wind', 'Shear', 'CL_HT', 'ctt', 'CAPE_shear', 'RAIN', 'WIND_sfc', 'sfc_theta_wind', 'helicity', 'pvo', 'avo', 'cin', 'updraft_helicity', 'slp', 'lcl', 'avo_500', 'pvo_500', 'avo_700', 'pvo_700', 'z_300', 'z_500', 'z_700', 'z_850', 'td2']\n",
    "cmaps = ['turbo', 'binary', 'turbo', 'turbo', 'grey_to_white', 'binary', 'turbo', 'GnBu', 'turbo', 'turbo', 'seismic', 'turbo', 'seismic', 'PuBu', 'inferno', 'RdBu_r', 'YlGn_r', 'seismic', 'turbo', 'seismic', 'turbo', 'RdBu_r', 'RdBu_r', 'RdBu_r', 'RdBu_r', 'YlGnBu'] \n",
    "bottoms = [1, None, None, None, None, -80, 1, .01, None, None, -500, -40, -100, 0, 10, 995, None, -100, -40, -100, -40, None, None, None, None, 45]\n",
    "tops = [None, None, None, 60, None, 20, None, 10, None, None, 500, 40, 100, 400, 110, 1020, None, 100, 40, 100, 40, None, None, None, None, 60]\n",
    "scales = [None, None, 9/5, 1.94384, None, None, None, None, 1.94384, None, None, None, None, None, None, None, None, None, None, None, None, .1, .1, .1, .1, 9/5]\n",
    "shifts = [None, None, -459.67, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 32]\n",
    "barbscales = [None, None, 1.94384, 1.94384, None, None, 1.94384, None, 1.94384, 1.94384, None, None, None, None, None, None, None, None, None, None, None, 1.94384, 1.94384, 1.94384, 1.94384, 1.94384]\n",
    "\n",
    "\n",
    "indices = [25]\n",
    "if indices is not None:\n",
    "    vars_of_interest = [vars_of_interest[i] for i in indices]\n",
    "    titles = [titles[i] for i in indices]\n",
    "    x_barbs = [x_barbs[i] for i in indices]\n",
    "    y_barbs = [y_barbs[i] for i in indices]\n",
    "    save_folders = [save_folders[i] for i in indices]\n",
    "    cmaps = [cmaps[i] for i in indices]\n",
    "    bottoms = [bottoms[i] for i in indices]\n",
    "    tops = [tops[i] for i in indices]\n",
    "    scales = [scales[i] for i in indices]\n",
    "    shifts = [shifts[i] for i in indices]\n",
    "    barbscales = [barbscales[i] for i in indices]\n",
    "\n",
    "i = 0\n",
    "tot = len(vars_of_interest)\n",
    "for var, title, x_barb, y_barb, scale, shift, barbscale, save_folder, cmap, bottom, top in zip(vars_of_interest, titles, x_barbs, y_barbs, scales, shifts, barbscales, save_folders, cmaps, bottoms, tops):\n",
    "    i += 1\n",
    "    print(save_folder, i, 'of', tot)\n",
    "    make_maps(ds, var, title, x_barb, y_barb, scale = scale, shift = shift, barbscale = barbscale, bottom = bottom, top = top, cmap = cmap, save = True, save_loc = 'images/hires/' + save_folder + '/', feature_ds = masks, feature_id = 6)\n",
    "    make_gif('images/hires/'+ save_folder)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc28b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "td2 1 of 1\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n"
     ]
    }
   ],
   "source": [
    "# zoomed in plots\n",
    "zoom = [-124, -119, 45, 49]\n",
    "thin = 10\n",
    "\n",
    "vars_of_interest = ['REFL_10CM_sfc', 'OLR', 'T2', 'mag_shear_6', 'CL_HT', 'ctt', 'cape', 'RAIN', 'WIND_sfc_mag', 'theta_sfc', 'helicity', 'pvo_sfc', 'avo_sfc', 'cin', 'updraft_helicity', 'slp', 'lcl', 'avo_500', 'pvo_500', 'avo_700', 'pvo_700', 'td2'] \n",
    "titles = ['Reflectivity (DBZ)', 'Outgoing Longwave Radiation (W/m2)', \n",
    "          '2m Temperature (F) and Wind Barbs', '0-6 km Wind Shear (Magnitude in kt and Barbs)', 'Cloud Top Height (m; Derived from Brightness Temp)', \n",
    "          'Cloud Top Temperature (degC)', 'CAPE (J/kg) and 0-6 km Wind Shear Barbs', 'Hourly Rain (mm)', 'Surface Wind (kt)', 'Surface Potential Temperature (K) and Wind Barbs',\n",
    "          '0-3 km Storm Relative Helicity (m2/s2)', 'Potential Vorticity (PVU)', 'Absolute Vorticity (10-5 s-1)', 'CIN (J/kg)', 'Updraft Helicity (m2/s2)', 'Sea Level Pressure (hPa) and Surface Wind Barbs', \n",
    "          'Lifting Condensation Level (m)', '500 mb Absolute Vorticity (10-5 s-1)', '500 mb Potential Vorticity (PVU)', '700 mb Absolute Vorticity (10-5 s-1)', '700 mb Potential Vorticity (PVU)', '2m Dew Point (F) and Surface Wind Barbs']\n",
    "\n",
    "x_barbs = [None, None, 'U10', 'U_shear_6', None, None, 'U_shear_6', None, 'U10', 'U10', None, None, None, None, None, 'U10', None, None, None, None, None, 'U10']\n",
    "y_barbs = [None, None,  'V10', 'V_shear_6', None, None, 'V_shear_6', None, 'V10', 'V10', None, None, None, None, None, 'V10', None, None, None, None, None, 'V10']\n",
    "save_folders = ['REFL_10CM',  'OLR', 'sfc_temp_wind', 'Shear', 'CL_HT', 'ctt', 'CAPE_shear', 'RAIN', 'WIND_sfc', 'sfc_theta_wind', 'helicity', 'pvo', 'avo', 'cin', 'updraft_helicity', 'slp', 'lcl', 'avo_500', 'pvo_500', 'avo_700', 'pvo_700', 'td2']\n",
    "cmaps = ['turbo', 'binary', 'turbo', 'turbo', 'grey_to_white', 'binary', 'turbo', 'GnBu', 'turbo', 'turbo', 'seismic', 'turbo', 'seismic', 'PuBu', 'inferno', 'RdBu_r', 'YlGn_r', 'seismic', 'turbo', 'seismic', 'turbo', 'YlGnBu'] \n",
    "bottoms = [1, None, None, None, None, -80, 1, .01, None, None, -500, -40, -100, 0, 10, 995, None, -100, -40, -100, -40, 45]\n",
    "tops = [None, None, None, 60, None, 20, None, 10, None, None, 500, 40, 100, 400, 110, 1020, None, 100, 40, 100, 40, 60]\n",
    "scales = [None, None, 9/5, 1.94384, None, None, None, None, 1.94384, None, None, None, None, None, None, None, None, None, None, None, None, 9/5]\n",
    "shifts = [None, None, -459.67, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 32]\n",
    "barbscales = [None, None, 1.94384, 1.94384, None, None, 1.94384, None, 1.94384, 1.94384, None, None, None, None, None, None, None, None, None, None, None, None]\n",
    "\n",
    "\n",
    "indices = [21]\n",
    "if indices is not None:\n",
    "    vars_of_interest = [vars_of_interest[i] for i in indices]\n",
    "    titles = [titles[i] for i in indices]\n",
    "    x_barbs = [x_barbs[i] for i in indices]\n",
    "    y_barbs = [y_barbs[i] for i in indices]\n",
    "    save_folders = [save_folders[i] for i in indices]\n",
    "    cmaps = [cmaps[i] for i in indices]\n",
    "    bottoms = [bottoms[i] for i in indices]\n",
    "    tops = [tops[i] for i in indices]\n",
    "    scales = [scales[i] for i in indices]\n",
    "    shifts = [shifts[i] for i in indices]\n",
    "    barbscales = [barbscales[i] for i in indices]\n",
    "\n",
    "i = 0\n",
    "tot = len(vars_of_interest)\n",
    "for var, title, x_barb, y_barb, scale, shift, barbscale, save_folder, cmap, bottom, top in zip(vars_of_interest, titles, x_barbs, y_barbs, scales, shifts, barbscales, save_folders, cmaps, bottoms, tops):\n",
    "    i += 1\n",
    "    print(save_folder, i, 'of', tot)\n",
    "    make_maps(ds, var, title, x_barb, y_barb, scale = scale, shift = shift, barbscale = barbscale, bottom = bottom, top = top, cmap = cmap, save = True, save_loc = 'images/zoom/' + save_folder + '/', zoom = zoom, thin = thin, feature_ds = masks, feature_id = 6)\n",
    "    make_gif('images/zoom/'+ save_folder)\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
